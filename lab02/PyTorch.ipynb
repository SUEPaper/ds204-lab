{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch 有何业务？\n",
    "\n",
    "在本课之前的学习当中，我们已经编写了大量代码来完成一套神经网络功能。Dropout、Batch Norm 和 2D 卷积是计算机视觉深度学习的一些主力。我们还努力使代码高效且矢量化。\n",
    "\n",
    "不过，对于本次作业的后一部分，我们将放弃您漂亮的代码库，而是迁移到两个流行的深度学习框架之一：在本例中，是 PyTorch（或 TensorFlow，如果您切换到该Jupyter Notebook）。 \n",
    "\n",
    "### 什么是 PyTorch?\n",
    "\n",
    "PyTorch 是一个用于在 Tensor 对象上执行动态计算图的系统，其行为与 Numpy Ndarray 类似。它配备了强大的自动微分引擎，无需手动反向传播。\n",
    "\n",
    "### 为什么用PyTorch?\n",
    "\n",
    "* 我们的代码现在将在 GPU 上运行！训练速度更快。使用 PyTorch 或 TensorFlow 等框架时，您可以利用 GPU 的强大功能来实现您自己的自定义神经网络架构，而无需直接编写 CUDA 代码（这超出了本课程的范围）。\n",
    "* 我们希望您准备好在您的项目中使用这些框架之一，这样您就可以比手动编写要使用的每个功能更有效地进行实验。\n",
    "* 我们希望你站在巨人的肩膀上！ TensorFlow 和 PyTorch 都是优秀的框架，可以让您的生活变得更加轻松，现在您了解了它们的本质，就可以自由使用它们了🙂\n",
    "* 我们希望您能够接触到在学术界或工业界可能遇到的深度学习代码。\n",
    "\n",
    "### PyTorch 版本\n",
    "\n",
    "本笔记本假设您使用 **PyTorch 0.4 版本**.在此版本之前，张量必须包装在 Variable 对象中才能在 Autograd 中使用；然而，变量现在已被弃用。此外，0.4 还将 Tensor 的数据类型与其设备分离，并使用 Numpy 风格的工厂来构造 Tensor，而不是直接调用 Tensor 构造函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 我们将如何学习 PyTorch?\n",
    "\n",
    "Justin Johnson 为PyTorch做了杰出贡献[教程](https://github.com/jcjohnson/pytorch-examples)。\n",
    "\n",
    "你还可以在这里找到详细的[API文档](http://pytorch.org/docs/stable/index.html) 。 如果您有 API 文档未解决的其他问题,[PyTorch forum](https://discuss.pytorch.org/) 是一个比 StackOverflow 更好的提问地点。\n",
    "\n",
    "\n",
    "# 目录\n",
    "\n",
    "本作业有 5 个部分。您将在不同的抽象级别上学习 PyTorch，这将帮助您更好地理解它并为最终项目做好准备。\n",
    "\n",
    "1. 准备工作: 我们将使用 CIFAR-10 数据集.\n",
    "2. Barebons PyTorch: 我们将直接使用最低级别的 PyTorch 张量。 \n",
    "3. PyTorch Module API: 我们将使用 `nn.Module` 来定义任意神经网络架构。\n",
    "4. PyTorch Sequential API: 我们将使用 `nn.Sequential` 非常方便地定义线性前馈网络。\n",
    "5. CIFAR-10 开放式挑战: 请实现您自己的网络，以在 CIFAR-10 上获得尽可能高的准确性。您可以尝试任何层、优化器、超参数或其他高级功能。\n",
    "\n",
    "这是一个比较表:\n",
    "\n",
    "| API           | 灵活性 | 便捷度 |\n",
    "|---------------|-------------|-------------|\n",
    "| Barebone      | 高        | 低         |\n",
    "| `nn.Module`     | 高        | 中      |\n",
    "| `nn.Sequential` | 低         | 高        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I. 准备工作\n",
    "首先，我们加载 CIFAR-10 数据集。第一次执行此操作可能需要几分钟，但此后文件应保留在缓存中。\n",
    "\n",
    "在作业的前面部分中，我们必须编写自己的代码来下载 CIFAR-10 数据集、对其进行预处理并以小批量方式迭代它； PyTorch 提供了方便的工具来为我们自动化这个过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./ds204-lab/datasets\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 170498071/170498071 [12:36<00:00, 225376.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./ds204-lab/datasets\\cifar-10-python.tar.gz to ./ds204-lab/datasets\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "NUM_TRAIN = 49000\n",
    "\n",
    "# torchvision.transforms 包提供了用于预处理数据的工具以及用于执行数据增强；在这里我们设置了一个转换\n",
    "# 通过减去平均 RGB 值并除以每个RGB值的标准差；我们已经对平均值和标准差进行了硬编码。\n",
    "transform = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            ])\n",
    "\n",
    "# 我们为每个分割（train / val / test）设置一个Dataset对象；数据集一次加载一个训练示例\n",
    "# 因此，我们将每个数据集包装在 DataLoader 中，该数据加载器迭代数据集并形成小批量。\n",
    "# 我们通过将 Sampler 对象传递给 CIFAR-10 训练集分为训练集和验证集。\n",
    "# DataLoader 告诉它应该如何从底层数据集中采样。\n",
    "cifar10_train = dset.CIFAR10('./ds204-lab/datasets', train=True, download=True,\n",
    "                             transform=transform)\n",
    "loader_train = DataLoader(cifar10_train, batch_size=64, \n",
    "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "cifar10_val = dset.CIFAR10('./ds204-lab/datasets', train=True, download=True,\n",
    "                           transform=transform)\n",
    "loader_val = DataLoader(cifar10_val, batch_size=64, \n",
    "                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
    "\n",
    "cifar10_test = dset.CIFAR10('./ds204-lab/datasets', train=False, download=True, \n",
    "                            transform=transform)\n",
    "loader_test = DataLoader(cifar10_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您可以选择 **通过将下面USE_GPU的标志设置为 True 来使用 GPU**. 此任务无需使用 GPU。请注意，如果您的计算机未启用CUDA , `torch.cuda.is_available()` 将返回 False，并且此笔记本将回退到 CPU 模式。\n",
    "\n",
    "全局变量 `dtype` 和 `device`将控制整个分配过程中的数据类型。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II. Barebones PyTorch\n",
    "\n",
    "PyTorch 附带了高级 API，可以帮助我们方便地定义模型架构，我们将在本教程的第二部分中介绍这一点。在本节中，我们将从准系统 PyTorch 元素开始，以更好地了解 Autograd 引擎。完成本次练习后，您将更加了解高级模型 API。\n",
    "\n",
    "我们将从一个简单的全连接 ReLU 网络开始，该网络具有两个隐藏层并且对 CIFAR 分类没有偏差。\n",
    "此实现使用 PyTorch 张量上的操作来计算前向传递，并使用 PyTorch Autograd 来计算梯度。理解每一行很重要，因为在示例之后您将编写一个更难的版本。\n",
    "\n",
    "当我们创建 PyTorch 张量时 `requires_grad=True`,那么涉及该张量的运算将不仅仅是计算值；他们还将在后台建立一个计算图，使我们能够轻松地通过该图进行反向传播，以计算一些张量相对于下游损失的梯度。 具体来说，如果 x 是一个张量 `x.requires_grad == True` 那么在反向传播之后，`x.grad` 将是另一个张量，它保存着 x 相对于最后的标量损失的梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch 张量(Tensor): 展平函数\n",
    "PyTorch 张量在概念上类似于 numpy 数组：它是一个 n 维数字网格，并且与 numpy 一样，PyTorch 提供了许多函数来有效地对张量进行操作。作为一个简单的例子，我们提供了一个 `(展平)flatten` 函数，它可以重塑图像数据以用于全连接的神经网络。\n",
    "\n",
    "回想一下，图像数据通常存储在形状为 N x C x H x W 的张量中，其中：\n",
    "\n",
    "* N 是数据点的数量\n",
    "* C是通道数\n",
    "* H 是中间特征图的高度（以像素为单位）\n",
    "* W 是中间特征图的宽度（以像素为单位）\n",
    "\n",
    "当我们进行 2D 卷积之类的操作时，这是表示数据的正确方法，这需要对中间特征彼此相对的位置进行空间理解。 然而，当我们使用全连接的仿射层来处理图像时，我们希望每个数据点都由单个向量表示 - 隔离数据的不同通道、行和列不再有用。 因此，我们使用“展平”操作将每个表示的“C x H x W”值折叠为单个长向量。 下面的 flatten 函数首先从给定的一批数据中读取 N、C、H 和 W 值，然后返回该数据的“视图”。 “View”类似于 numpy 的“reshape”方法：它将 x 的维度重塑为 N x ??，其中 ??允许是任何东西（在本例中，它将是 C x H x W，但我们不需要明确指定）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before flattening:  tensor([[[[ 0,  1],\n",
      "          [ 2,  3],\n",
      "          [ 4,  5]]],\n",
      "\n",
      "\n",
      "        [[[ 6,  7],\n",
      "          [ 8,  9],\n",
      "          [10, 11]]]])\n",
      "After flattening:  tensor([[ 0,  1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "def flatten(x):\n",
    "    N = x.shape[0] # 读取 N, C, H, W\n",
    "    return x.view(N, -1)  # \"展平\"  C * H * W 的值，转化为每个图像的单个向量\n",
    "\n",
    "def test_flatten():\n",
    "    x = torch.arange(12).view(2, 1, 3, 2)\n",
    "    print('Before flattening: ', x)\n",
    "    print('After flattening: ', flatten(x))\n",
    "\n",
    "test_flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barebones PyTorch: 双层网络\n",
    "\n",
    "这里我们定义了一个函数 `two_layer_fc`，它对一批图像数据执行两层全连接 ReLU 网络的前向传递。定义前向传递后，我们检查它是否不会崩溃，并且通过在网络中运行零来生成正确形状的输出。\n",
    "\n",
    "您不必在这里编写任何代码，但阅读并理解实现非常重要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F  # 有用的无状态函数\n",
    "\n",
    "def two_layer_fc(x, params):\n",
    "    \"\"\"\n",
    "    全连接的神经网络；\n",
    "    其框架是:\n",
    "    NN 为  全连接-> ReLU -> 全连接层。\n",
    "    注意这个函数只定义了前向传播；\n",
    "    PyTorch 将为我们处理向后传递。\n",
    "    \n",
    "    网络的输入将是一小批数据， 其形状\n",
    "    (N, d1, ..., dM) 其中 d1 * ... * dM = D。 T隐藏层将有 H 个单元，\n",
    "    输出层将产生 C 类的分数。\n",
    "    \n",
    "    输入:\n",
    "    - x: 形状为 (N, d1, ..., dM) 的 PyTorch 张量，给出的小批量输入数据。\n",
    "    - params:PyTorch 张量列表 [w1, w2] 为网络提供权重；w1 具有形状 (D, H)，w2 具有形状 (H, C)。\n",
    "    \n",
    "    返回:\n",
    "    - scores: 形状为 (N, C) 的 PyTorch 张量，给出输入数据x的分类分数。\n",
    "    \"\"\"\n",
    "    # 首先我们展平图像\n",
    "    x = flatten(x)  # shape: [batch_size, C x H x W]\n",
    "    \n",
    "    w1, w2 = params\n",
    "    \n",
    "    # 前向传递: 使用张量运算计算预测 y. w1和w2的 requires_grad=True，\n",
    "    # 涉及这些Tensors的操作会导致PyTorch 构建计算图，允许自动计算梯度\n",
    "    # 因此我们不再手动实现向后传递 ，我们不需要保留对中间值的引用。\n",
    "    # 你也可以使用`.clamp(min=0)`, 相当于F.relu()\n",
    "    x = F.relu(x.mm(w1))\n",
    "    x = x.mm(w2)\n",
    "    return x\n",
    "    \n",
    "\n",
    "def two_layer_fc_test():\n",
    "    hidden_layer_size = 42\n",
    "    x = torch.zeros((64, 50), dtype=dtype)  # 小批量大小 64,特征维度 50\n",
    "    w1 = torch.zeros((50, hidden_layer_size), dtype=dtype)\n",
    "    w2 = torch.zeros((hidden_layer_size, 10), dtype=dtype)\n",
    "    scores = two_layer_fc(x, [w1, w2])\n",
    "    print(scores.size())  # 你应该看到输出为 [64, 10]\n",
    "\n",
    "two_layer_fc_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barebones PyTorch: 三层卷积网络\n",
    "\n",
    "在这里，您将完成函数 `three_layer_convnet`的实现, 该函数将执行三层卷积网络的向前传递。 像上面一样，我们可以通过网络传递零来立即测试我们的实现。网络应具有以下架构：\n",
    "1. 带有`channel_1`滤波器的卷积层（带有偏差），每个滤波器的形状为`KW1 x KH1`，以及两个零填充\n",
    "2. 非线性ReLU\n",
    "3. 带有`channel_2`滤波器的卷积层（带有偏差），每个滤波器的形状为`KW2 x KH2`，以及两个零填充\n",
    "4. 非线性ReLU\n",
    "5. 带偏差的全连接层，为 C 类生成分数。\n",
    "\n",
    "**提示**: 对于卷积: http://pytorch.org/docs/stable/nn.html#torch.nn.functional.conv2d; 注意卷积滤波器的形状！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_layer_convnet(x, params):\n",
    "    \"\"\"\n",
    "    使用上面定义的架构执行三层卷积网络的前向传递。\n",
    "\n",
    "    输入:\n",
    "    - x: 提供小批量图像形状 (N, 3, H, W) 的 PyTorch 张量，\n",
    "    - params: 给出网络权重和偏差的 PyTorch 张量列表；\n",
    "    应包含以下内容：\n",
    "      - conv_w1：形状 (channel_1, 3, KH1, KW1) 的 PyTorch 张量，给出第一个卷积层的权重\n",
    "      - conv_b1: 形状 (channel_1,) 的 PyTorch 张量给出第一个卷积层的偏差\n",
    "      - conv_w2: 形状 (channel_2, channel_1, KH2, KW2) 的 PyTorch 张量，给出第二个卷积层的权重\n",
    "      - conv_b2: 形状 (channel_2,) 的 PyTorch 张量给出第二个卷积层的偏差\n",
    "      - fc_w: PyTorch Tensor 为全连接层赋予权重。你能猜出它应该是什么形状吗？\n",
    "      - fc_b: PyTorch Tensor 为全连接层提供偏差。你能猜出它应该是什么形状吗？\n",
    "    \n",
    "    返回:\n",
    "    - scores: 形状 (N, C) 的 PyTorch 张量给出 x 的分类分数\n",
    "    \"\"\"\n",
    "    conv_w1, conv_b1, conv_w2, conv_b2, fc_w, fc_b = params\n",
    "    scores = None\n",
    "    ################################################################################\n",
    "    # TODO: 实现三层 ConvNet 的前向传递。                                           #\n",
    "    ################################################################################\n",
    "    #       *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****          #\n",
    "    ################################################################################\n",
    "    \n",
    "  \n",
    "    conv1 = F.conv2d(x, weight=conv_w1, bias=conv_b1, padding=2)\n",
    "    relu1 = F.relu(conv1)\n",
    "    conv2 = F.conv2d(relu1, weight=conv_w2, bias=conv_b2, padding=1)\n",
    "    relu2 = F.relu(conv2)\n",
    "    relu2_flat = flatten(relu2)\n",
    "    scores = relu2_flat.mm(fc_w) + fc_b\n",
    "    \n",
    "    ################################################################################\n",
    "    #                                END OF YOUR CODE                              #\n",
    "    ################################################################################\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义上述 ConvNet 的前向传递后，运行以下单元来测试您的实现。\n",
    "\n",
    "当您运行此函数时，scores的形状应为 (64, 10)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "def three_layer_convnet_test():\n",
    "    x = torch.zeros((64, 3, 32, 32), dtype=dtype)  # 小批量大小 64, 图片大小 [3, 32, 32]\n",
    "\n",
    "    conv_w1 = torch.zeros((6, 3, 5, 5), dtype=dtype)  # [out_channel, in_channel, kernel_H, kernel_W]\n",
    "    conv_b1 = torch.zeros((6,))  # out_channel\n",
    "    conv_w2 = torch.zeros((9, 6, 3, 3), dtype=dtype)  # [out_channel, in_channel, kernel_H, kernel_W]\n",
    "    conv_b2 = torch.zeros((9,))  # out_channel\n",
    "\n",
    "    # 您必须在两个转换层之后、全连接层之前计算张量的形状\n",
    "    fc_w = torch.zeros((9 * 32 * 32, 10))\n",
    "    fc_b = torch.zeros(10)\n",
    "\n",
    "    scores = three_layer_convnet(x, [conv_w1, conv_b1, conv_w2, conv_b2, fc_w, fc_b])\n",
    "    print(scores.size())  # 你应该看到[64, 10]\n",
    "three_layer_convnet_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barebones PyTorch: 初始化\n",
    "让我们编写几个实用方法来初始化模型的权重矩阵。\n",
    "\n",
    "- `random_weight(shape)` 使用 Kaiming 归一化方法初始化权重张量。\n",
    "- `zero_weight(shape)` 用全零初始化权重张量。对于实例化偏置参数很有用。\n",
    "\n",
    "`random_weight` 函数使用Kaiming普通初始化方法，描述如下\n",
    "\n",
    "He et al, *Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification*, ICCV 2015, https://arxiv.org/abs/1502.01852"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4758, -0.7288, -0.7309, -0.6499,  0.1560],\n",
       "        [ 0.1188,  0.2502, -0.4702,  0.5564, -0.2477],\n",
       "        [-1.1646, -1.2569,  0.6985, -0.6833, -1.2493]], device='cuda:0',\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def random_weight(shape):\n",
    "    \"\"\"\n",
    "   为权重创建随机张量；设置 requires_grad=True 意味着我们\n",
    "    想要在向后传递过程中计算这些张量的梯度。\n",
    "    我们使用 Kaiming 归一化: sqrt(2 / fan_in)\n",
    "    \"\"\"\n",
    "    if len(shape) == 2:  # FC weight\n",
    "        fan_in = shape[0]\n",
    "    else:\n",
    "        fan_in = np.prod(shape[1:]) # conv weight [out_channel, in_channel, kH, kW]\n",
    "    # randn 是标准正态分布生成器。 \n",
    "    w = torch.randn(shape, device=device, dtype=dtype) * np.sqrt(2. / fan_in)\n",
    "    w.requires_grad = True\n",
    "    return w\n",
    "\n",
    "def zero_weight(shape):\n",
    "    return torch.zeros(shape, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "# 创建权重的形状 [3 x 5]\n",
    "# 如果您使用 GPU，您应该会看到`torch.cuda.FloatTensor`类型。\n",
    "# 否则它应该是  `torch.FloatTensor`\n",
    "random_weight((3, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barebones PyTorch: 检查准确性\n",
    "训练模型时，我们将使用以下函数来检查模型在训练或验证集上的准确性。\n",
    "\n",
    "检查准确性时，我们不需要计算任何梯度；因此，当我们计算分数时，我们不需要 PyTorch 为我们构建计算图。为了防止构建图表，我们将计算范围限制在`torch.no_grad()`上下文管理器下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy_part2(loader, model_fn, params):\n",
    "    \"\"\"\n",
    "   检查分类模型的准确性。\n",
    "    \n",
    "    输入:\n",
    "    - loader:用于我们要检查的数据分割的 DataLoader\n",
    "    - model_fn:执行模型前向传递的函数，签名为 Score = model_fn(x, params)\n",
    "    - params: 给出模型参数的 PyTorch 张量列表\n",
    "    \n",
    "    返回:Nothing, 但打印模型的准确性\n",
    "    \"\"\"\n",
    "    split = 'val' if loader.dataset.train else 'test'\n",
    "    print('Checking accuracy on the %s set' % split)\n",
    "    num_correct, num_samples = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # 更换设备，如GPU\n",
    "            y = y.to(device=device, dtype=torch.int64)\n",
    "            scores = model_fn(x, params)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f%%)' % (num_correct, num_samples, 100 * acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BareBones PyTorch: 训练循环\n",
    "我们现在可以建立一个基本的训练循环来训练我们的网络。我们将使用无动量的随机梯度下降来训练模型。我们将使用`torch.function.cross_entropy`来计算损失； 你可以 [在这里阅读相关内容](http://pytorch.org/docs/stable/nn.html#cross-entropy).\n",
    "\n",
    "训练循环将神经网络函数、初始化参数列表（在我们的示例中为`[w1, w2]`）和学习率作为输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_part2(model_fn, params, learning_rate):\n",
    "    \"\"\"\n",
    "    在 CIFAR-10 上训练模型。\n",
    "    \n",
    "    输入:\n",
    "    - model_fn:执行模型前向传播的 Python 函数。它应该具有签名 Score = model_fn(x, params) \n",
    "      其中 x 是图像数据的 PyTorch 张量，params 是 PyTorch 张量的列表模型权重，\n",
    "      分数是形状 (N, C) 的 PyTorch 张量 x 中元素的分数。\n",
    "    - params:赋予模型权重的 PyTorch 张量列表\n",
    "    - learning_rate: Python 标量给出用于 SGD 的学习率\n",
    "    \n",
    "    返回: Nothing\n",
    "    \"\"\"\n",
    "    for t, (x, y) in enumerate(loader_train):\n",
    "        # 将数据移动到正确的设备（GPU 或 CPU）\n",
    "        x = x.to(device=device, dtype=dtype)\n",
    "        y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "        # 前向传递：计算分数和损失\n",
    "        scores = model_fn(x, params)\n",
    "        loss = F.cross_entropy(scores, y)\n",
    "\n",
    "        # 向后传递：PyTorch 找出计算图中哪些张量具有 require_grad=True \n",
    "        # 并使用反向传播来计算相对于这些张量的损失梯度，\n",
    "        # 并将梯度存储在每个 Tensor 的 .grad 属性中。\n",
    "        loss.backward()\n",
    "\n",
    "        # 更新参数。 \n",
    "        #我们不想通过参数更新进行反向传播，\n",
    "        #因此我们将更新范围限制在 torch.no_grad() 上下文管理器下，以防止构建计算图。\n",
    "        with torch.no_grad():\n",
    "            for w in params:\n",
    "                w -= learning_rate * w.grad\n",
    "\n",
    "                # 运行反向传播后手动将梯度归零\n",
    "                w.grad.zero_()\n",
    "\n",
    "        if t % print_every == 0:\n",
    "            print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "            check_accuracy_part2(loader_val, model_fn, params)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BareBones PyTorch: 训练两层网络\n",
    "现在我们准备好运行训练循环了。我们需要为全连接权重`w1`和`w2`显式分配张量。 \n",
    "\n",
    "CIFAR 的每个小批量有 64 个示例，因此张量形状为 `[64, 3, 32, 32]`。\n",
    "\n",
    "展平后, `x` 的形状应为`[64, 3 * 32 * 32]`. 他将是`w1`第一个维度的大小。 \n",
    "`w1`的第二个维度是隐藏层大小，这也是`w2`的第一个维度。 \n",
    "\n",
    "最后，网络的输出是一个 10 维向量，表示 10 个类别的概率分布。\n",
    "\n",
    "您不需要调整任何超参数，但在训练一个 epoch 后，您应该会看到准确率高于 40%。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 3.1638\n",
      "Checking accuracy on the val set\n",
      "Got 141 / 1000 correct (14.10%)\n",
      "\n",
      "Iteration 100, loss = 2.0802\n",
      "Checking accuracy on the val set\n",
      "Got 345 / 1000 correct (34.50%)\n",
      "\n",
      "Iteration 200, loss = 2.5873\n",
      "Checking accuracy on the val set\n",
      "Got 364 / 1000 correct (36.40%)\n",
      "\n",
      "Iteration 300, loss = 2.0787\n",
      "Checking accuracy on the val set\n",
      "Got 382 / 1000 correct (38.20%)\n",
      "\n",
      "Iteration 400, loss = 2.0624\n",
      "Checking accuracy on the val set\n",
      "Got 419 / 1000 correct (41.90%)\n",
      "\n",
      "Iteration 500, loss = 1.7178\n",
      "Checking accuracy on the val set\n",
      "Got 417 / 1000 correct (41.70%)\n",
      "\n",
      "Iteration 600, loss = 2.2978\n",
      "Checking accuracy on the val set\n",
      "Got 427 / 1000 correct (42.70%)\n",
      "\n",
      "Iteration 700, loss = 1.3888\n",
      "Checking accuracy on the val set\n",
      "Got 405 / 1000 correct (40.50%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hidden_layer_size = 4000\n",
    "learning_rate = 1e-2\n",
    "\n",
    "w1 = random_weight((3 * 32 * 32, hidden_layer_size))\n",
    "w2 = random_weight((hidden_layer_size, 10))\n",
    "\n",
    "train_part2(two_layer_fc, [w1, w2], learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BareBones PyTorch: 训练卷积网络\n",
    "\n",
    "在下面，您应该使用上面定义的函数在 CIFAR 上训练三层卷积网络。网络应具有以下架构：\n",
    "\n",
    "1. 具有 32 个 5x5 滤波器的卷积层（带偏置），零填充为 2\n",
    "2. ReLU\n",
    "3. 具有 16 个 3x3 滤波器的卷积层（带偏置），零填充为 1\n",
    "4. ReLU\n",
    "5. 全连接层（带有偏差）计算 10 个类别的分数\n",
    "\n",
    "您应该使用上面定义的`random_weight`函数初始化权重矩阵，并且应该使用上面定义的`zero_weight`函数初始化偏差向量。\n",
    "\n",
    "您不需要调整任何超参数，但如果一切正常，您应该在一个 epoch 后达到 42% 以上的准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 3.9098\n",
      "Checking accuracy on the val set\n",
      "Got 115 / 1000 correct (11.50%)\n",
      "\n",
      "Iteration 100, loss = 1.7671\n",
      "Checking accuracy on the val set\n",
      "Got 360 / 1000 correct (36.00%)\n",
      "\n",
      "Iteration 200, loss = 1.4617\n",
      "Checking accuracy on the val set\n",
      "Got 399 / 1000 correct (39.90%)\n",
      "\n",
      "Iteration 300, loss = 1.9257\n",
      "Checking accuracy on the val set\n",
      "Got 447 / 1000 correct (44.70%)\n",
      "\n",
      "Iteration 400, loss = 1.7655\n",
      "Checking accuracy on the val set\n",
      "Got 451 / 1000 correct (45.10%)\n",
      "\n",
      "Iteration 500, loss = 1.4304\n",
      "Checking accuracy on the val set\n",
      "Got 486 / 1000 correct (48.60%)\n",
      "\n",
      "Iteration 600, loss = 1.4907\n",
      "Checking accuracy on the val set\n",
      "Got 478 / 1000 correct (47.80%)\n",
      "\n",
      "Iteration 700, loss = 1.5136\n",
      "Checking accuracy on the val set\n",
      "Got 486 / 1000 correct (48.60%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 3e-3\n",
    "\n",
    "channel_1 = 32\n",
    "channel_2 = 16\n",
    "\n",
    "conv_w1 = None\n",
    "conv_b1 = None\n",
    "conv_w2 = None\n",
    "conv_b2 = None\n",
    "fc_w = None\n",
    "fc_b = None\n",
    "\n",
    "################################################################################\n",
    "# TODO: 初始化三层ConvNet的参数。                                              #\n",
    "################################################################################\n",
    "#       *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****          #\n",
    "################################################################################\n",
    "\n",
    "conv_w1 = random_weight((channel_1, 3, 5, 5))\n",
    "conv_b1 = zero_weight((channel_1,))\n",
    "conv_w2 = random_weight((channel_2, 32, 3, 3))\n",
    "conv_b2 = zero_weight((channel_2,))\n",
    "fc_w = random_weight((channel_2*32*32, 10))\n",
    "fc_b = zero_weight((10,))\n",
    "\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             #\n",
    "################################################################################\n",
    "\n",
    "params = [conv_w1, conv_b1, conv_w2, conv_b2, fc_w, fc_b]\n",
    "train_part2(three_layer_convnet, params, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III. PyTorch Module API\n",
    "\n",
    "Barebone PyTorch 要求我们手动跟踪所有参数张量。这对于具有几个张量的小型网络来说很好，但在较大的网络中跟踪数十或数百个张量将非常不方便且容易出错。\n",
    "\n",
    "PyTorch 提供`nn.Module` API 供您定义任意网络架构，同时为您跟踪每个可学习的参数。 在Part II中, 我们自己实施了SGD(梯度下降). PyTorch  还提供了  `torch.optim` 包，它实现了所有常见的优化器， 例如 RMSProp, Adagrad 和 Adam. 它甚至支持近似二阶方法，如 L-BFGS！您可以参考[文档](http://pytorch.org/docs/master/optim.html)了解每个优化器的具体规格。\n",
    "\n",
    "要使用模块 API，请按照以下步骤操作：\n",
    "\n",
    "1. 子类`nn.Module`。为您的网络类指定一个直观的名称，例如`TwoLayerFC`。\n",
    "\n",
    "2. 在构造函数`__init__()`中，将所需的所有层定义为类属性。像`nn.Linear`和`nn.Conv2d`这样的图层对象本身就是`nn.Module`子类，并且包含可学习的参数， 这样您就不必自己实例化原始张量。 `nn.Module` 将为您跟踪这些内部参数。 请参阅 [文档](http://pytorch.org/docs/master/nn.html)以了解有关数十个内置层的更多信息。 **警告**: 不要忘记先调用 `super().__init__()` ！\n",
    "\n",
    "3. 在`forward()`方法中，定义网络的*连接性*。 您应该使用 __init__ 中定义的属性作为函数调用，以张量作为输入并输出“转换后的”张量。*不要* 在`forward()`中创建具有可学习参数的任何新层！所有这些都必须在“__init__”中预先声明。\n",
    "\n",
    "定义模块子类后，您可以将其实例化为对象并调用它,就像Part II中的 NN 前向函数一样。\n",
    "\n",
    "### Module API:双层网络\n",
    "下面是一个 2 层全连接网络的具体示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "class TwoLayerFC(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        # 将图层对象分配给类属性\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        # nn.init 包含便捷的初始化方法\n",
    "        # http://pytorch.org/docs/master/nn.html#torch-nn-init \n",
    "        nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # forward 始终定义连接性\n",
    "        x = flatten(x)\n",
    "        scores = self.fc2(F.relu(self.fc1(x)))\n",
    "        return scores\n",
    "\n",
    "def test_TwoLayerFC():\n",
    "    input_size = 50\n",
    "    x = torch.zeros((64, input_size), dtype=dtype)  # 小批量大小 64, 特征维度 50\n",
    "    model = TwoLayerFC(input_size, 42, 10)\n",
    "    scores = model(x)\n",
    "    print(scores.size())  # 你应该看到 [64, 10]\n",
    "test_TwoLayerFC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module API: 三层卷积网络\n",
    "现在轮到你实现一个 3 层 ConvNet，然后是一个全连接层。网络架构应该与Part II相似:\n",
    "\n",
    "1. 具有“channel_1”5x5 滤波器的卷积层，零填充为 2\n",
    "2. ReLU\n",
    "3. 具有“channel_2” 3x3 滤波器的卷积层，零填充为 1\n",
    "4. ReLU\n",
    "5. 到“num_classes”类的全连接层\n",
    "\n",
    "您应该使用 Kaiming 法线初始化方法来初始化模型的权重矩阵。\n",
    "\n",
    "**提示**: http://pytorch.org/docs/stable/nn.html#conv2d\n",
    "\n",
    "实现三层 ConvNet 后, `test_ThreeLayerConvNet` 函数将运行您的实现; 它应该打印`(64, 10)`作为输出分数的形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "class ThreeLayerConvNet(nn.Module):\n",
    "    def __init__(self, in_channel, channel_1, channel_2, num_classes):\n",
    "        super().__init__()\n",
    "        ########################################################################\n",
    "        # TODO: 使用上面定义的架构设置三层 ConvNet 所需的层                      #\n",
    "        ########################################################################\n",
    "        #*****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****         #\n",
    "        ########################################################################\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channel, channel_1, kernel_size=5, padding=2, bias=True)\n",
    "        nn.init.kaiming_normal_(self.conv1.weight)\n",
    "        nn.init.constant_(self.conv1.bias, 0)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(channel_1, channel_2, kernel_size=3, padding=1, bias=True)\n",
    "        nn.init.kaiming_normal_(self.conv2.weight)\n",
    "        nn.init.constant_(self.conv2.bias, 0)\n",
    "        \n",
    "        self.fc = nn.Linear(channel_2*32*32, num_classes)\n",
    "        nn.init.kaiming_normal_(self.fc.weight)\n",
    "        nn.init.constant_(self.fc.bias, 0)\n",
    "        \n",
    "        ########################################################################\n",
    "        #                          END OF YOUR CODE                            #       \n",
    "        ########################################################################\n",
    "\n",
    "    def forward(self, x):\n",
    "        scores = None\n",
    "        ########################################################################\n",
    "        # TODO: 实现 3 层 ConvNet 的前向功能。您应该使用在 __init__ 中定义的层   #\n",
    "        # 并在forward() 中指定这些层的连接                                      #\n",
    "        ########################################################################\n",
    "        #*****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****         #\n",
    "        ########################################################################\n",
    "        \n",
    "        relu1 = F.relu(self.conv1(x))\n",
    "        relu2 = F.relu(self.conv2(relu1))\n",
    "        scores = self.fc(flatten(relu2))\n",
    "        \n",
    "        ########################################################################\n",
    "        #                          END OF YOUR CODE                            #\n",
    "        ########################################################################\n",
    "        return scores\n",
    "\n",
    "\n",
    "def test_ThreeLayerConvNet():\n",
    "    x = torch.zeros((64, 3, 32, 32), dtype=dtype)  # 小批量大小 64, 图片大小 [3, 32, 32]\n",
    "    model = ThreeLayerConvNet(in_channel=3, channel_1=12, channel_2=8, num_classes=10)\n",
    "    scores = model(x)\n",
    "    print(scores.size())  # 你应该看到 [64, 10]\n",
    "test_ThreeLayerConvNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module API: 检查准确性\n",
    "给定验证或测试集，我们可以检查神经网络的分类准确性。\n",
    "\n",
    "这个版本与第二部分的版本略有不同。您不再手动传递参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy_part34(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')   \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # 将模型设置为评估模式\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # 转换设备，例如GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module API: 训练循环\n",
    "我们还使用了稍微不同的训练循环, 我们不是自己更新权重值，而是使用`torch.optim`包中的 Optimizer 对象，该对象抽象了优化算法的概念，并提供了大多数常用于优化神经网络的算法的实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_part34(model, optimizer, epochs=1):\n",
    "    \"\"\"\n",
    "    使用 PyTorch 模块 API 在 CIFAR-10 上训练模型。\n",
    "    \n",
    "    输入:\n",
    "    - model: 提供模型训练的 PyTorch Model。\n",
    "    - optimizer: 我们将使用一个 Optimizer 对象来训练模型。\n",
    "    - epochs: (可选) 一个 Python 整数，给定要训练的次数\n",
    "    \n",
    "    返回: Nothing, 但在训练期间打印模型精度。\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # 将模型参数移至CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train()  # 将模型置于训练模式\n",
    "            x = x.to(device=device, dtype=dtype)  # 转换设备，例如GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            # 将优化器将更新的变量的所有梯度归零。\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 这是向后传递：计算模型每个参数的损失梯度。\n",
    "            loss.backward()\n",
    "\n",
    "            # 实际上使用向后传递计算的梯度来更新模型的参数。\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % print_every == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "                check_accuracy_part34(loader_val, model)\n",
    "                print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module API: 训练双层网络\n",
    "现在我们准备好运行训练循环了。与Part II相反，我们不再显式分配参数张量。\n",
    "\n",
    "只需将输入大小、隐藏层大小和类数（即输出大小）传递给`TwoLayerFC`的构造函数即可。\n",
    "\n",
    "您还需要定义一个优化器来跟踪`TwoLayerFC`内的所有可学习参数。\n",
    "\n",
    "您不需要调整任何超参数，但在训练一个 epoch 后，您应该会看到模型准确率高于 40%。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 3.2762\n",
      "Checking accuracy on validation set\n",
      "Got 119 / 1000 correct (11.90)\n",
      "\n",
      "Iteration 100, loss = 2.6811\n",
      "Checking accuracy on validation set\n",
      "Got 332 / 1000 correct (33.20)\n",
      "\n",
      "Iteration 200, loss = 1.9052\n",
      "Checking accuracy on validation set\n",
      "Got 381 / 1000 correct (38.10)\n",
      "\n",
      "Iteration 300, loss = 1.9003\n",
      "Checking accuracy on validation set\n",
      "Got 386 / 1000 correct (38.60)\n",
      "\n",
      "Iteration 400, loss = 1.8123\n",
      "Checking accuracy on validation set\n",
      "Got 422 / 1000 correct (42.20)\n",
      "\n",
      "Iteration 500, loss = 2.0043\n",
      "Checking accuracy on validation set\n",
      "Got 360 / 1000 correct (36.00)\n",
      "\n",
      "Iteration 600, loss = 1.8639\n",
      "Checking accuracy on validation set\n",
      "Got 442 / 1000 correct (44.20)\n",
      "\n",
      "Iteration 700, loss = 1.9156\n",
      "Checking accuracy on validation set\n",
      "Got 466 / 1000 correct (46.60)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hidden_layer_size = 4000\n",
    "learning_rate = 1e-2\n",
    "model = TwoLayerFC(3 * 32 * 32, hidden_layer_size, 10)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_part34(model, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module API: 训练三层卷积网络\n",
    "您现在应该使用模块 API 在 CIFAR 上训练三层 ConvNet。这看起来应该与训练两层网络非常相似！你不需要调整任何超参数，但训练一个 epoch 后应该达到 45% 以上。\n",
    "\n",
    "您应该使用没有动量的随机梯度下降来训练模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 3.8929\n",
      "Checking accuracy on validation set\n",
      "Got 144 / 1000 correct (14.40)\n",
      "\n",
      "Iteration 100, loss = 1.9717\n",
      "Checking accuracy on validation set\n",
      "Got 348 / 1000 correct (34.80)\n",
      "\n",
      "Iteration 200, loss = 1.8484\n",
      "Checking accuracy on validation set\n",
      "Got 394 / 1000 correct (39.40)\n",
      "\n",
      "Iteration 300, loss = 1.6329\n",
      "Checking accuracy on validation set\n",
      "Got 420 / 1000 correct (42.00)\n",
      "\n",
      "Iteration 400, loss = 1.8622\n",
      "Checking accuracy on validation set\n",
      "Got 466 / 1000 correct (46.60)\n",
      "\n",
      "Iteration 500, loss = 1.7899\n",
      "Checking accuracy on validation set\n",
      "Got 457 / 1000 correct (45.70)\n",
      "\n",
      "Iteration 600, loss = 1.5297\n",
      "Checking accuracy on validation set\n",
      "Got 476 / 1000 correct (47.60)\n",
      "\n",
      "Iteration 700, loss = 1.3070\n",
      "Checking accuracy on validation set\n",
      "Got 497 / 1000 correct (49.70)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 3e-3\n",
    "channel_1 = 32\n",
    "channel_2 = 16\n",
    "\n",
    "model = None\n",
    "optimizer = None\n",
    "################################################################################\n",
    "# TODO: 实例化您的 ThreeLayerConvNet 模型和相应的优化器                          #\n",
    "################################################################################\n",
    "#*****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****                 #\n",
    "################################################################################\n",
    "\n",
    "model = ThreeLayerConvNet(3, channel_1, channel_2, 10)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "################################################################################\n",
    "#                             END OF YOUR CODE                                 #\n",
    "################################################################################\n",
    "\n",
    "train_part34(model, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part IV. PyTorch Sequential API\n",
    "\n",
    "Part III 引入了 PyTorch Module API, 它允许您定义任意可学习层及其连接性。\n",
    "\n",
    "对于简单模型，例如一堆前馈层, 你还需要完成3个步骤: 子类 `nn.Module`, 在`__init__`中将层分配给类属性，并在`forward()`中逐层调用每一层。 \n",
    "\n",
    "有没有更便捷的方法呢？\n",
    "\n",
    "幸运的是, PyTorch 提供了一个名为 `nn.Sequential`的容器模块，, 它将上述步骤合并为一个. 它不像`nn.Module`那么灵活，因为您无法指定比前馈堆栈更复杂的拓扑，但它对于许多用例来说已经足够了。\n",
    "\n",
    "### Sequential API: 双层网络\n",
    "让我们看看如何使用`nn.Sequential`重写我们的两层全连接网络示例，并使用上面定义的训练循环来训练它。\n",
    "\n",
    "同样，您不需要在这里调整任何超参数，但经过一个 epoch 的训练后，您应该达到 40% 以上的准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 2.3243\n",
      "Checking accuracy on validation set\n",
      "Got 174 / 1000 correct (17.40)\n",
      "\n",
      "Iteration 100, loss = 1.8203\n",
      "Checking accuracy on validation set\n",
      "Got 390 / 1000 correct (39.00)\n",
      "\n",
      "Iteration 200, loss = 1.7088\n",
      "Checking accuracy on validation set\n",
      "Got 388 / 1000 correct (38.80)\n",
      "\n",
      "Iteration 300, loss = 1.6694\n",
      "Checking accuracy on validation set\n",
      "Got 406 / 1000 correct (40.60)\n",
      "\n",
      "Iteration 400, loss = 1.8396\n",
      "Checking accuracy on validation set\n",
      "Got 426 / 1000 correct (42.60)\n",
      "\n",
      "Iteration 500, loss = 1.9302\n",
      "Checking accuracy on validation set\n",
      "Got 422 / 1000 correct (42.20)\n",
      "\n",
      "Iteration 600, loss = 1.8641\n",
      "Checking accuracy on validation set\n",
      "Got 423 / 1000 correct (42.30)\n",
      "\n",
      "Iteration 700, loss = 1.5718\n",
      "Checking accuracy on validation set\n",
      "Got 446 / 1000 correct (44.60)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 我们需要将`flatten`函数包装在模块中，以便将其堆叠在 nn.Sequential 中\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return flatten(x)\n",
    "\n",
    "hidden_layer_size = 4000\n",
    "learning_rate = 1e-2\n",
    "\n",
    "model = nn.Sequential(\n",
    "    Flatten(),\n",
    "    nn.Linear(3 * 32 * 32, hidden_layer_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_layer_size, 10),\n",
    ")\n",
    "\n",
    "# 您可以在 optim.SGD 中使用 Nesterov 动量\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                     momentum=0.9, nesterov=True)\n",
    "\n",
    "train_part34(model, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential API: 三层卷积网络\n",
    "在这里，您应该使用`nn.Sequential`来定义和训练一个三层 ConvNet，其架构与我们在Part III中使用的架构相同：\n",
    "\n",
    "1. 具有 32 个 5x5 滤波器的卷积层（带偏置），零填充为 2\n",
    "2. ReLU\n",
    "3. 具有 16 个 3x3 滤波器的卷积层（带偏置），零填充为 1\n",
    "4. ReLU\n",
    "5. 全连接层（带有偏差）计算 10 个类别的分数\n",
    "\n",
    "您应该使用上面定义的`random_weight`函数初始化权重矩阵，并且应该使用上面定义的`zero_weight`函数初始化偏差向量。\n",
    "\n",
    "您应该使用 Nesterov 动量 0.9 的随机梯度下降来优化您的模型。\n",
    "\n",
    "同样，您不需要调整任何超参数，但经过一轮训练后您应该会看到准确率高于 55%。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaiming_normal(shape):\n",
    "    \"\"\"\n",
    "    为权重创建随机张量；设置 require_grad=True 意味着我们要在向后传递过程中计算这些张量的梯度。\n",
    "    我们使用 Kaiming 初始化: sqrt(2 / fan_in)\n",
    "    \"\"\"\n",
    "    if len(shape) == 2:  # FC weight\n",
    "        fan_in = shape[1]  # 与 `random_weight()` 不同，pytorch 中 nn.Linear 的权重形状为：[out_feature, in_feature]\n",
    "    else:\n",
    "        fan_in = np.prod(shape[1:]) # 转换权重 [out_channel, in_channel, kH, kW]\n",
    "    # randn 是标准正态分布生成器。\n",
    "    w = torch.randn(shape, device=device, dtype=dtype) * np.sqrt(2. / fan_in)\n",
    "    w.requires_grad = True\n",
    "    return w\n",
    "\n",
    "def xavier_normal(shape):\n",
    "    \"\"\"\n",
    "    为权重创建随机张量；设置 require_grad=True 意味着我们要在向后传递过程中计算这些张量的梯度。\n",
    "    我们使用 Xavier 初始化: sqrt(2 / (fan_in + fan_out))\n",
    "    \"\"\"\n",
    "    if len(shape) == 2:  # FC weight\n",
    "        fan_in = shape[1]\n",
    "        fan_out = shape[0]\n",
    "    else:\n",
    "        fan_in = np.prod(shape[1:]) # 转换权重 [out_channel, in_channel, kH, kW]\n",
    "        fan_out = shape[0] * shape[2] * shape[3]\n",
    "    # randn 是标准正态分布生成器。\n",
    "    w = torch.randn(shape, device=device, dtype=dtype) * np.sqrt(2. / (fan_in + fan_out))\n",
    "    w.requires_grad = True\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 2.4132\n",
      "Checking accuracy on validation set\n",
      "Got 158 / 1000 correct (15.80)\n",
      "\n",
      "Iteration 100, loss = 1.6887\n",
      "Checking accuracy on validation set\n",
      "Got 429 / 1000 correct (42.90)\n",
      "\n",
      "Iteration 200, loss = 1.3527\n",
      "Checking accuracy on validation set\n",
      "Got 487 / 1000 correct (48.70)\n",
      "\n",
      "Iteration 300, loss = 1.4333\n",
      "Checking accuracy on validation set\n",
      "Got 489 / 1000 correct (48.90)\n",
      "\n",
      "Iteration 400, loss = 1.4937\n",
      "Checking accuracy on validation set\n",
      "Got 524 / 1000 correct (52.40)\n",
      "\n",
      "Iteration 500, loss = 1.2148\n",
      "Checking accuracy on validation set\n",
      "Got 528 / 1000 correct (52.80)\n",
      "\n",
      "Iteration 600, loss = 1.2533\n",
      "Checking accuracy on validation set\n",
      "Got 545 / 1000 correct (54.50)\n",
      "\n",
      "Iteration 700, loss = 1.1077\n",
      "Checking accuracy on validation set\n",
      "Got 572 / 1000 correct (57.20)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "channel_1 = 32\n",
    "channel_2 = 16\n",
    "learning_rate = 1e-2\n",
    "\n",
    "model = None\n",
    "optimizer = None\n",
    "\n",
    "################################################################################\n",
    "# TODO:使用 Sequential API 重写第三部分中带有偏差的 3 层 ConvNet。               #                                                        #\n",
    "################################################################################\n",
    "#*****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****                 #\n",
    "################################################################################\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, channel_1, kernel_size=5, padding=2),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(channel_1, channel_2, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    Flatten(),\n",
    "    nn.Linear(channel_2*32*32, 10),\n",
    ")\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                     momentum=0.9, nesterov=True)\n",
    "\n",
    "# 权重初始化\n",
    "# 参考: http://pytorch.org/docs/stable/nn.html#torch.nn.Module.apply\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Conv2d or type(m) == nn.Linear:\n",
    "#         m.weight.data = random_weight(m.weight.size())\n",
    "#         m.weight.data = kaiming_normal(m.weight.size())\n",
    "        m.weight.data = xavier_normal(m.weight.size())\n",
    "        m.bias.data = zero_weight(m.bias.size())\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "################################################################################\n",
    "#                            END OF YOUR CODE                                  #                           \n",
    "################################################################################\n",
    "\n",
    "train_part34(model, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part V. CIFAR-10 开放式挑战\n",
    "\n",
    "在本节中，您可以在 CIFAR-10 上试验您想要的任何 ConvNet 架构。\n",
    "\n",
    "现在，您的工作是试验架构、超参数、损失函数和优化器，以训练一个模型，使其在 10 个 epoch 内的 CIFAR-10 **验证集** 上实现 **至少 70%** 的准确度。您可以使用上面的check_accuracy 和 train 函数。您可以使用`nn.Module`或`nn.Sequential`API。\n",
    "\n",
    "描述一下你在本笔记本末尾做了什么。\n",
    "\n",
    "以下是每个组件的官方 API 文档。需要注意的是：我们在`空间批规范`类中所说的在 PyTorch 中称为`BatchNorm2D`。\n",
    "\n",
    "* torch.nn 包中的层: http://pytorch.org/docs/stable/nn.html\n",
    "* 激活函数: http://pytorch.org/docs/stable/nn.html#non-linear-activations\n",
    "* 损失函数: http://pytorch.org/docs/stable/nn.html#loss-functions\n",
    "* Optimizers优化器: http://pytorch.org/docs/stable/optim.html\n",
    "\n",
    "\n",
    "### 你可以尝试的事情:\n",
    "- **过滤器尺寸**: 上面我们使用了5x5；较小的过滤器是否会更有效？\n",
    "- **过滤器数量**: 上面我们使用了 32 个过滤器。是越多越好还是越少越好？\n",
    "- **池化与跨步卷积**: 您使用最大池化还是仅使用跨步卷积？\n",
    "- **批量归一化**: 尝试在卷积层之后添加空间批量归一化，并在仿射层之后添加普通批量归一化。您的网络训练得更快吗？\n",
    "- **网络架构**: 上面的网络有两层可训练参数。你能用深度网络做得更好吗？值得尝试的良好架构包括：\n",
    "    - [卷积-Relu-池化]xN -> [仿射]xM -> [softmax or SVM]\n",
    "    - [卷积-Relu-卷积-Relu-池化]xN -> [仿射]xM -> [softmax 或 SVM]\n",
    "    - [批量归一化-Relu-卷积]xN -> [仿射]xM -> [softmax 或 SVM]\n",
    "- **全局平均池**: 不要展平然后拥有多个仿射层，而是执行卷积直到图像变小（7x7 左右），然后执行平均池化操作以获得 1x1 图像图片（1, 1 , Filter#），然后将其重新整形为一个 (Filter#) 向量。 这在谷歌的初始网络[Google's Inception Network](https://arxiv.org/abs/1512.00567)中使用 (其架构见表 1).\n",
    "- **正则化**: 添加 l2 权重正则化，或者可能使用 Dropout。\n",
    "### 关于训练的小提示\n",
    "对于您尝试的每个网络架构，您应该调整学习率和其他超参数。执行此操作时，需要记住以下重要事项：\n",
    "\n",
    "- 如果参数运行良好，您应该会在几百次迭代内看到改进\n",
    "- 请记住超参数调整的从粗到细的方法：首先测试大量的超参数，进行几次训练迭代，以找到有效的参数组合。\n",
    "- 一旦找到一些似乎有效的参数集，就可以围绕这些参数进行更精细的搜索。您可能需要训练更多次数。\n",
    "- 您应该使用验证集进行超参数搜索，并保存测试集，以便根据验证集选择的最佳参数评估您的架构。\n",
    "\n",
    "### 改进、提高！\n",
    "如果您喜欢探索，您可以实施许多其他功能来尝试提高您的表现。 **不需要**实现其中任何一个，但如果您有时间，请不要错过其中的乐趣！\n",
    "\n",
    "- 替代优化器：你可以尝试 Adam、Adagrad、RMSprop 等。\n",
    "- 替代激活函数，例如leaky ReLU、参数化ReLU、ELU 或MaxOut。\n",
    "- Model整合\n",
    "- 数据增强\n",
    "- 新架构\n",
    "  - [ResNets](https://arxiv.org/abs/1512.03385) 其中前一层的输入被添加到输出中。\n",
    "  - [DenseNets](https://arxiv.org/abs/1608.06993) 其中前一层的输入连接在一起。\n",
    "  - [This blog has an in-depth overview](https://chatbotslife.com/resnets-highwaynets-and-densenets-oh-my-9bb15918ee32)\n",
    "\n",
    "### 享受愉快、快乐的训练! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 2.4207\n",
      "Checking accuracy on validation set\n",
      "Got 105 / 1000 correct (10.50)\n",
      "\n",
      "Iteration 0, loss = 0.8765\n",
      "Checking accuracy on validation set\n",
      "Got 628 / 1000 correct (62.80)\n",
      "\n",
      "Iteration 0, loss = 0.7449\n",
      "Checking accuracy on validation set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "\n",
      "Iteration 0, loss = 0.6328\n",
      "Checking accuracy on validation set\n",
      "Got 702 / 1000 correct (70.20)\n",
      "\n",
      "Iteration 0, loss = 0.6688\n",
      "Checking accuracy on validation set\n",
      "Got 718 / 1000 correct (71.80)\n",
      "\n",
      "Iteration 0, loss = 0.5392\n",
      "Checking accuracy on validation set\n",
      "Got 754 / 1000 correct (75.40)\n",
      "\n",
      "Iteration 0, loss = 0.6220\n",
      "Checking accuracy on validation set\n",
      "Got 723 / 1000 correct (72.30)\n",
      "\n",
      "Iteration 0, loss = 0.4938\n",
      "Checking accuracy on validation set\n",
      "Got 760 / 1000 correct (76.00)\n",
      "\n",
      "Iteration 0, loss = 0.4196\n",
      "Checking accuracy on validation set\n",
      "Got 770 / 1000 correct (77.00)\n",
      "\n",
      "Iteration 0, loss = 0.6582\n",
      "Checking accuracy on validation set\n",
      "Got 748 / 1000 correct (74.80)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# TODO:                                                                        #         \n",
    "#  使用任何架构、优化器和超参数进行实验。                                        #\n",
    "#  在 10 个周期内的“验证集”上实现至少 70% 的准确率。                             #\n",
    "#  请注意，您可以使用 check_accuracy 函数对测试集或验证集进行评估                 #\n",
    "#  方法是将 loader_test 或 loader_val 作为第二个参数传递给 check_accuracy。      #\n",
    "#  在完成架构和超参数调整之前，不应触摸测试集                                    # \n",
    "#  并且仅在最后运行测试集一次以报告最终值。                                      #\n",
    "################################################################################\n",
    "#*****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****                 #\n",
    "################################################################################\n",
    "model = None\n",
    "optimizer = None\n",
    "\n",
    "# 4层卷积网络\n",
    "# (卷积 -> 批量归一化 -> Relu -> max池化) * 3 -> fc\n",
    "layer1 = nn.Sequential(\n",
    "    nn.Conv2d(3, 16, kernel_size=5, padding=2),\n",
    "    nn.BatchNorm2d(16),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2)\n",
    ")\n",
    "\n",
    "layer2 = nn.Sequential(\n",
    "    nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2)\n",
    ")\n",
    "\n",
    "layer3 = nn.Sequential(\n",
    "    nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2)\n",
    ")\n",
    "\n",
    "fc = nn.Linear(64*4*4, 10)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    layer1,\n",
    "    layer2,\n",
    "    layer3,\n",
    "    Flatten(),\n",
    "    fc\n",
    ")\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 每个时期打印训练状态：将 print_every 设置为一个大数字\n",
    "print_every = 10000\n",
    "\n",
    "################################################################################\n",
    "#                                END OF YOUR CODE                              #                       \n",
    "################################################################################\n",
    "\n",
    "# 您应该获得至少 70% 的准确率\n",
    "train_part34(model, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 描述一下你做了什么\n",
    "\n",
    "在下面的单元格中，您应该解释您所做的事情、您实现的任何附加功能和/或您在训练和评估网络的过程中制作的任何图表。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: 描述一下你做了什么"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试集——仅运行一次\n",
    "\n",
    "现在我们已经得到了满意的结果，我们在测试集上测试我们的最终模型（您应该将其存储在 best_model 中）。考虑一下这与您的验证集准确性相比如何。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on test set\n",
      "Got 7331 / 10000 correct (73.31)\n"
     ]
    }
   ],
   "source": [
    "best_model = model\n",
    "check_accuracy_part34(loader_test, best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
