{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "286a1b33",
   "metadata": {},
   "source": [
    "# 从零开始构建你的第一个神经网络\n",
    "\n",
    "在这个笔记本中，您将实现构建深度神经网络所需的所有函数。\n",
    "\n",
    "**完成此lab后，你可以实现如下功能:**\n",
    "- 构建一个更深的神经网络（具有超过1个隐藏层）\n",
    "- 实现一个易于使用的神经网络类\n",
    "\n",
    "**符号说明**：\n",
    "- 上标 $[l]$ 表示与第 $l$ 层相关的量。\n",
    "    - 例如：$a^{[L]}$ 是第 $L$ 层的激活值。$W^{[L]}$ 和 $b^{[L]}$ 是第 $L$ 层的参数。\n",
    "- 上标 $(i)$ 表示与第 $i$ 个示例相关的量。\n",
    "    - 例如：$x^{(i)}$ 是第 $i$ 个训练样本。\n",
    "- 下标 $i$ 表示向量的第 $i$ 个条目。\n",
    "    - 例如：$a^{[l]}_i$ 表示第 $l$ 层激活值的第 $i$ 个条目。\n",
    "    \n",
    "让我们开始吧！！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2004cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537089a1",
   "metadata": {},
   "source": [
    "# 1 - 全连接（仿射）层\n",
    "\n",
    "## 1.1 - 线性前向传播\n",
    "\n",
    "您将从实现一些基础函数开始，稍后在实现模型时将会用到这些函数。您需要按照以下顺序完成三个函数：\n",
    "\n",
    "- LINEAR\n",
    "- LINEAR -> ACTIVATION，其中 ACTIVATION 将是 ReLU 或 Sigmoid 中的一个。\n",
    "- [LINEAR -> RELU] × (L-1) -> LINEAR -> SIGMOID（完整模型）\n",
    "\n",
    "线性前向模块（针对所有示例的向量化）计算以下方程式：\n",
    "\n",
    "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{4}$$\n",
    "\n",
    "其中 $A^{[0]} = X$。\n",
    "\n",
    "**练习**：构建前向传播的线性部分。请实现 `layers.py`中的 `linear_forward` 函数。\n",
    "\n",
    "**提示**：\n",
    "这个单元的数学表示是 $Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}$。您可能会发现 `np.dot()` 函数很有用。如果您的维度不匹配，打印 `W.shape` 可能会有帮助。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ed58e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import linear_forward\n",
    "from test_cases import linear_forward_test_case\n",
    "\n",
    "A, W, b = linear_forward_test_case()\n",
    "\n",
    "Z, linear_cache = linear_forward(A, W, b)\n",
    "print(\"Z = \" + str(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b93e06",
   "metadata": {},
   "source": [
    "**预期得到的输出**:\n",
    "\n",
    "```\n",
    "Z = [[ 3.26295337 -1.23429987]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ca08a9",
   "metadata": {},
   "source": [
    "## 1.2 - 线性反向传播\n",
    "\n",
    "对于第 $l$ 层，线性部分是：$Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$（其后是激活函数）。\n",
    "\n",
    "假设您已经计算出了导数 $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$。您想要得到 $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$。\n",
    "\n",
    "<img src=\"images/linearback_kiank.png\" style=\"width:250px;height:300px;\">\n",
    "<caption><center> **图 4** </center></caption>\n",
    "\n",
    "三个输出 $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$ 使用输入 $dZ^{[l]}$ 来计算。以下是您需要的公式：\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{8}$$\n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{9}$$\n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\tag{10}$$\n",
    "\n",
    "在执行线性反向传播时，您将使用这些公式来计算梯度。在实际的代码实现中，您将使用这些导数来更新参数（权重 $W^{[l]}$ 和偏置 $b^{[l]}$），以及计算前一层的梯度 $dA^{[l-1]}$，这对于实现多层网络中的反向传播至关重要。\n",
    "\n",
    "**练习**：请实现 `layers.py`中的 `linear_backward` 函数。\n",
    "\n",
    "**提示**：\n",
    "您可能会发现 `np.dot()` 和 `np.sum()` 函数很有用。如果您的维度不匹配，打印 `W.shape` 可能会有帮助。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e89e76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import linear_backward\n",
    "from test_cases import linear_backward_test_case\n",
    "\n",
    "dZ, linear_cache = linear_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "print(\"dA_prev = \"+ str(dA_prev))\n",
    "print(\"dW = \" + str(dW))\n",
    "print(\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f3b8b4",
   "metadata": {},
   "source": [
    "**预期得到的输出**:\n",
    "    \n",
    "```\n",
    "dA_prev = \n",
    " [[ 0.51822968 -0.19517421]\n",
    " [-0.40506361  0.15255393]\n",
    " [ 2.37496825 -0.89445391]]\n",
    "dW = \n",
    " [[-0.10076895  1.40685096  1.64992505]]\n",
    "db = \n",
    " [[0.50629448]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a195374f",
   "metadata": {},
   "source": [
    "# 2 - 激活函数\n",
    "\n",
    "## 2.1 - Sigmoid 函数\n",
    "\n",
    "### 2.1.1 - Sigmoid 前向传播\n",
    "\n",
    "**Sigmoid 函数**：$\\sigma(Z) = \\sigma(W A + b) = \\frac{1}{1 + e^{-(W A + b)}}$。\n",
    "\n",
    "**练习**：请实现 `layers.py`中的 `sigmoid_forward` 函数。\n",
    "\n",
    "**提示**：我们为您提供了 `sigmoid_forward` 函数。这个函数返回**两个**项：激活值 \"`a`\" 和一个包含 \"`Z`\" 的 \"`cache`\"（这是我们将输入到相应的反向函数中的内容）。您可能会发现 `np.exp()` 函数很有用。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5c18cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import sigmoid_forward\n",
    "from test_cases import sigmoid_forward_test_case\n",
    "\n",
    "Z = sigmoid_forward_test_case()\n",
    "\n",
    "A, cache = sigmoid_forward(Z)\n",
    "\n",
    "print(\"A = \" + str(A))\n",
    "print(\"cache = \" + str(cache))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd0614c",
   "metadata": {},
   "source": [
    "**预期得到的输出**:\n",
    "    \n",
    "```\n",
    "A = [[0.39729283 0.485937   0.10562821]\n",
    " [0.83757178 0.14265203 0.3011669 ]]\n",
    "cache = [[-0.41675785 -0.05626683 -2.1361961 ]\n",
    " [ 1.64027081 -1.79343559 -0.84174737]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5bcf5e",
   "metadata": {},
   "source": [
    "### 2.1.2 - Sigmoid backward\n",
    "\n",
    "Sigmoid 函数：$\\sigma(Z) = \\frac{1}{1 + e^{-Z}}$。\n",
    "\n",
    "其导数（对z的梯度）可以表示为：$\\frac{d\\sigma(z)}{dz} = \\sigma(z) (1 - \\sigma(z))$\n",
    "\n",
    "**练习**：请实现 `layers.py`中的 `sigmoid_backward` 函数。\n",
    "\n",
    "**提示**：您可能会发现 `np.exp()` 函数很有用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa26cf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import sigmoid_backward\n",
    "from test_cases import sigmoid_backward_test_case\n",
    "\n",
    "dA, cache = sigmoid_backward_test_case()\n",
    "\n",
    "dZ = sigmoid_backward(dA, cache)\n",
    "\n",
    "print(\"dZ = \" + str(dZ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263f38ed",
   "metadata": {},
   "source": [
    "**预期得到的输出**:\n",
    "    \n",
    "```\n",
    "dZ = dZ = [[-0.09787036 -0.00976551 -0.40863549]\n",
    " [ 0.335792   -0.41592828 -0.07015842]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdabd38a",
   "metadata": {},
   "source": [
    "## 2.2 Relu 函数\n",
    "\n",
    "### 2.2.1 Relu 函数前向传\n",
    "\n",
    "**ReLU**：ReLU的数学公式是 $A = RELU(Z) = \\max(0, Z)$。我们为您提供了 `relu` 函数。这个函数返回**两个**项：激活值 \"`A`\" 和一个包含 \"`Z`\" 的 \"`cache`\"（这是我们将输入到相应的反向函数中的内容）。\n",
    "\n",
    "**练习**：请实现 `layers.py`中的 `relu_forward` 函数。\n",
    "\n",
    "**提示**：您可能会发现 `np.maximum()` 函数很有用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d5aeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import relu_forward\n",
    "from test_cases import relu_forward_test_case\n",
    "\n",
    "Z = relu_forward_test_case()\n",
    "\n",
    "A, cache = relu_forward(Z)\n",
    "\n",
    "print(\"A = \" + str(A))\n",
    "print(\"cache = \" + str(cache))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ae2862",
   "metadata": {},
   "source": [
    "**预期得到的输出**:\n",
    "    \n",
    "```\n",
    "A = [[1.62434536 0.         0.        ]\n",
    " [0.         0.86540763 0.        ]]\n",
    "cache = [[ 1.62434536 -0.61175641 -0.52817175]\n",
    " [-1.07296862  0.86540763 -2.3015387 ]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007c1359",
   "metadata": {},
   "source": [
    "### 2.2.2 Relu 函数后向传播\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{d}{dz}ReLU(z)  = \n",
    "  \\begin{cases} \n",
    "   0 & \\text{if } z \\le 0 \\\\\n",
    "   1 & \\text{if } z > 0 \n",
    "  \\end{cases}\n",
    "$$\n",
    "\n",
    "**练习**：请实现 `layers.py`中的 `relu_backward` 函数。\n",
    "\n",
    "**提示**：您可能会发现 `np.array(_, copy=True) ` 函数很有用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e88d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import relu_backward\n",
    "from test_cases import relu_backward_test_case\n",
    "\n",
    "dA, cache = relu_backward_test_case()\n",
    "\n",
    "dZ = relu_backward(dA, cache)\n",
    "\n",
    "print(\"dZ = \" + str(dZ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d7ce5b",
   "metadata": {},
   "source": [
    "**预期得到的输出**:\n",
    "    \n",
    "```\n",
    "dZ = [[ 1.62434536  0.         -0.52817175]\n",
    " [ 0.          0.86540763  0.        ]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06f8077",
   "metadata": {},
   "source": [
    "# 3 - 线性-激活层\n",
    "\n",
    "## 3.1 - 前向传播\n",
    "\n",
    "为了更加方便，您将会把两个函数（线性和激活）合并为一个函数（LINEAR->ACTIVATION）。因此，您将实现一个函数，它先进行线性前向步骤，然后是激活前向步骤。\n",
    "\n",
    "**练习**：实现 *线性->激活* 层的前向传播。数学关系式为：$A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})$，其中激活函数 \"g\" 可以是 sigmoid 或 relu。请使用您之前实现的函数来实现`layers.py`中的 `linear_activation_forward` 函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91ca72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import linear_activation_forward\n",
    "from test_cases import linear_activation_forward_test_case\n",
    "\n",
    "A_prev, W, b = linear_activation_forward_test_case()\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n",
    "print(\"With sigmoid: A = \" + str(A))\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n",
    "print(\"With ReLU: A = \" + str(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28959d2e",
   "metadata": {},
   "source": [
    "**预期得到的输出**:\n",
    "    \n",
    "```\n",
    "With sigmoid: A = [[ 0.96890023  0.11013289]]\n",
    "With ReLU: A = [[ 3.43896131  0.        ]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4387b0b",
   "metadata": {},
   "source": [
    "## 3.2 - 反向传播\n",
    "\n",
    "如果 $g(.)$ 是激活函数，\n",
    "`sigmoid_backward` 和 `relu_backward` 计算 $dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}) \\tag{11}$。\n",
    "\n",
    "**练习**：实现 *线性->激活* 层的反向传播。请实现`layers.py`中的 `linear_activation_backward` 函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7608e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import linear_activation_backward\n",
    "from test_cases import linear_activation_backward_test_case\n",
    "\n",
    "\n",
    "dAL, linear_activation_cache = linear_activation_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"sigmoid\")\n",
    "print (\"sigmoid:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"relu\")\n",
    "print (\"relu:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffa2a10",
   "metadata": {},
   "source": [
    "**预期得到的输出**:\n",
    "    \n",
    "```\n",
    "sigmoid:\n",
    "dA_prev = [[ 0.11017994  0.01105339]\n",
    " [ 0.09466817  0.00949723]\n",
    " [-0.05743092 -0.00576154]]\n",
    "dW = [[ 0.10266786  0.09778551 -0.01968084]]\n",
    "db = [[-0.05729622]]\n",
    "\n",
    "relu:\n",
    "dA_prev = [[ 0.44090989 -0.        ]\n",
    " [ 0.37883606 -0.        ]\n",
    " [-0.2298228   0.        ]]\n",
    "dW = [[ 0.44513824  0.37371418 -0.10478989]]\n",
    "db = [[-0.20837892]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2fc144",
   "metadata": {},
   "source": [
    "# 4 - L-Layer Model \n",
    "\n",
    "## 4.1 - 前向传播\n",
    "\n",
    "\n",
    "为了在实现 \\(L\\) 层神经网络时更加方便，你需要一个函数，这个函数复制前面的函数（`linear_activation_forward` 与 Relu）\\(L-1\\) 次，然后跟随一个 `linear_activation_forward` 与 Sigmoid。\n",
    "\n",
    "<img src=\"images/model_architecture_kiank.png\" style=\"width:600px;height:300px;\">\n",
    "<caption><center> **图 2** : *[线性 -> Relu] \\( \\times (L-1) \\) -> 线性 -> Sigmoid* 模型</center></caption><br>\n",
    "\n",
    "**练习**：请您实现`layers.py`中的 ` L_model_forward` 函数。\n",
    "\n",
    "**说明**：在下面的代码中，变量 `AL` 将表示 \\(A^{[L]} = \\sigma(Z^{[L]}) = \\sigma(W^{[L]} A^{[L-1]} + b^{[L]})\\)。（这有时也被称为 `Yhat`，即这是 \\(\\hat{Y}\\)。）\n",
    "\n",
    "**提示**：\n",
    "- 使用你之前写过的函数\n",
    "- 使用 for 循环复制 [线性->RELU] \\(L-1\\) 次\n",
    "- 不要忘记在 \"caches\" 列表中跟踪缓存。要在 `list` 中添加一个新值 `c`，可以使用 `list.append(c)`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e83360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import L_model_forward\n",
    "from test_cases import L_model_forward_test_case_2hidden\n",
    "\n",
    "X, parameters = L_model_forward_test_case_2hidden()\n",
    "AL, caches = L_model_forward(X, parameters)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"Length of caches list = \" + str(len(caches)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f2a4d1",
   "metadata": {},
   "source": [
    "**预期得到的输出**:\n",
    "    \n",
    "```\n",
    "AL = [[ 0.03921668  0.70498921  0.19734387  0.04728177]]\n",
    "Length of caches list = 3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdf8ab7",
   "metadata": {},
   "source": [
    "## 4.2 - 后向传播\n",
    "\n",
    "与前向传播一样，您将实现反向传播的辅助函数。请记住，反向传播用于计算损失函数相对于参数的梯度。\n",
    "\n",
    "**提醒**：\n",
    "<img src=\"images/backprop_kiank.png\" style=\"width:650px;height:250px;\">\n",
    "<caption><center> **图 3** : *线性->RELU->线性->SIGMOID* 的前向和反向传播<br>*紫色块表示前向传播，红色块表示反向传播。*</center></caption>\n",
    "\n",
    "对于那些精通微积分的人（做这个作业不需要你是专家），可以使用微积分的链式法则导出二层网络中损失 \\(\\mathcal{L}\\) 相对于 \\(z^{[1]}\\) 的导数，如下所示：\n",
    "\n",
    "$$\\frac{d \\mathcal{L}(a^{[2]}, y)}{{dz^{[1]}}} = \\frac{d\\mathcal{L}(a^{[2]}, y)}{{da^{[2]}}}\\frac{{da^{[2]}}}{{dz^{[2]}}}\\frac{{dz^{[2]}}}{{da^{[1]}}}\\frac{{da^{[1]}}}{{dz^{[1]}}} \\tag{8} $$\n",
    "\n",
    "为了计算梯度 $dW^{[1]} = \\frac{\\partial \\mathcal{L}}{\\partial W^{[1]}}$，你使用上面的链式法则，并执行 $dW^{[1]} = dz^{[1]} \\times \\frac{\\partial z^{[1]} }{\\partial W^{[1]}}$。在反向传播期间，每一步你都会将当前梯度乘以对应层的梯度，以获取你想要的梯度。\n",
    "\n",
    "同样，为了计算梯度 $db^{[1]} = \\frac{\\partial \\mathcal{L}}{\\partial b^{[1]}}$，你使用上面的链式法则，并执行 $db^{[1]} = dz^{[1]} \\times \\frac{\\partial z^{[1]} }{\\partial b^{[1]}}$。\n",
    "\n",
    "这就是为什么我们讨论**反向传播**的原因。\n",
    "\n",
    "现在，类似于前向传播，您将在三个步骤中构建反向传播：\n",
    "- 线性反向\n",
    "- 线性 -> 激活反向，其中激活计算 ReLU 或 sigmoid 激活的导数\n",
    "- [线性 -> RELU] \\( \\times \\) (L-1) -> 线性 -> SIGMOID 反向（整个模型）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7326fa6a",
   "metadata": {},
   "source": [
    "现在你将为整个网络实现反向传播函数。回想一下，当你实现了 `L_model_forward` 函数时，你在每次迭代时都存储了一个包含 (X,W,b, 和 z) 的缓存。在反向传播模块中，你将使用这些变量来计算梯度。因此，在 `L_model_backward` 函数中，你将从第 \\(L\\) 层开始，向后遍历所有隐藏层。在每一步中，你将使用第 \\(l\\) 层的缓存值通过第 \\(l\\) 层反向传播。下面的图 5 显示了反向传播的过程。\n",
    "\n",
    "<img src=\"images/mn_backward.png\" style=\"width:450px;height:300px;\">\n",
    "<caption><center> **图 5**：反向传播 </center></caption>\n",
    "\n",
    "** 初始化反向传播**：\n",
    "为了通过这个网络反向传播，我们知道输出是\n",
    "$A^{[L]} = \\sigma(Z^{[L]})$。因此，你的代码需要计算 $dAL= \\frac{\\partial \\mathcal{L}}{\\partial A^{[L]}}$。\n",
    "为此，使用这个公式（使用微积分导出，不需要深入了解）：\n",
    "\n",
    "```python\n",
    "dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # # derivative of cost with respect to AL\n",
    "```\n",
    "\n",
    "\n",
    "然后，你可以使用这个激活后的梯度 $dAL$ 继续向后传播。如图 5 所示，你现在可以将 $dAL$ 输入到你实现的 LINEAR->SIGMOID 反向传播函数中（这将使用由 L_model_forward 函数存储的缓存值）。之后，你将需要使用一个 `for` 循环通过 LINEAR->RELU 反向传播函数遍历所有其他层。你应该将每个 $dA$, $dW$ 和 $db$ 存储在 grads 字典中。为此，使用以下公式：\n",
    "\n",
    "$$grads[\"dW\" + str(l)] = dW^{[l]}\\tag{15} $$\n",
    "\n",
    "例如，对于 $l=3$，这将在 `grads[\"dW3\"]` 中存储 $dW^{[l]}$。\n",
    "\n",
    "**练习**：为 *[LINEAR->RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID* 模型实现反向传播。请您实现`layers.py`中的 ` L_model_backward` 函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7522913",
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import L_model_backward\n",
    "from test_cases import L_model_backward_test_case, print_grads\n",
    "\n",
    "AL, Y_assess, caches = L_model_backward_test_case()\n",
    "grads = L_model_backward(AL, Y_assess, caches)\n",
    "print_grads(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea600248",
   "metadata": {},
   "source": [
    "**预期得到的输出**:\n",
    "    \n",
    "```\n",
    "dW1 = [[0.41010002 0.07807203 0.13798444 0.10502167]\n",
    " [0.         0.         0.         0.        ]\n",
    " [0.05283652 0.01005865 0.01777766 0.0135308 ]]\n",
    "db1 = [[-0.22007063]\n",
    " [ 0.        ]\n",
    " [-0.02835349]]\n",
    "dA1 = [[ 0.          0.52257901]\n",
    " [ 0.         -0.3269206 ]\n",
    " [ 0.         -0.32070404]\n",
    " [ 0.         -0.74079187]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801b3227",
   "metadata": {},
   "source": [
    "# 5 - 损失函数\n",
    "现在你将实现前向传播和反向传播。你需要计算损失函数，因为你想检查你的模型是否真的在学习。\n",
    "\n",
    "使用以下公式计算交叉熵损失：\n",
    "\n",
    "$J = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)) \\tag{7}$\n",
    "\n",
    "**练习**：请您实现`loss.py`中的 `cross_entropy_loss` 函数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014ed9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from loss import cross_entropy_loss\n",
    "from test_cases import cross_entropy_loss_test_case\n",
    "\n",
    "Y, AL = cross_entropy_loss_test_case()\n",
    "\n",
    "print(\"loss = \" + str(cross_entropy_loss(AL, Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9f44b2",
   "metadata": {},
   "source": [
    "**预期得到的输出**:\n",
    "    \n",
    "```\n",
    "loss = 0.41493159961539694\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a97525",
   "metadata": {},
   "source": [
    "# 6 - 初始化\n",
    "\n",
    "你将编写两个辅助函数来初始化模型的参数。第一个函数将用于初始化两层模型的参数。第二个将把这个初始化过程推广到 \\( L \\) 层。\n",
    "\n",
    "## 6.1 - 两层神经网络\n",
    "\n",
    "**练习**：创建并初始化两层神经网络的参数。请您实现`initial.py`中的 `initialize_parameters` 函数。\n",
    "\n",
    "**说明**：\n",
    "- 模型的结构是：*线性 -> ReLU -> 线性 -> Sigmoid*。\n",
    "- 对权重矩阵使用随机初始化。使用 `np.random.randn(shape)*0.01` 并给出正确的形状。\n",
    "- 对偏置使用零初始化。使用 `np.zeros(shape)`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c85c125",
   "metadata": {},
   "outputs": [],
   "source": [
    "from initial import initialize_parameters\n",
    "\n",
    "parameters = initialize_parameters(3,2,1)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f3aef4",
   "metadata": {},
   "source": [
    "**预期得到的输出**:\n",
    "    \n",
    "```\n",
    "W1 = [[ 0.01624345 -0.00611756 -0.00528172]\n",
    " [-0.01072969  0.00865408 -0.02301539]]\n",
    "b1 = [[ 0.]\n",
    " [ 0.]]\n",
    "W2 = [[ 0.01744812 -0.00761207]]\n",
    "b2 = [[ 0.]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29324a82",
   "metadata": {},
   "source": [
    "### 3.2 - L层神经网络\n",
    "\n",
    "一个更深层次的L层神经网络的初始化更加复杂，因为有更多的权重矩阵和偏置向量。在完成 `initialize_parameters_deep` 时，你应该确保每一层之间的维度是匹配的。回想一下 $ n^{[l]} $ 是第 $ l $ 层中的单元数量。因此，例如如果我们输入 $ X $ 的大小是 $ (12288, 209) $（有 $ m=209 $ 个样本），那么："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ee3051",
   "metadata": {},
   "source": [
    "|  |  **Shape of W** | **Shape of b** |  **Activation** | **Shape of Activation** |\n",
    "|  :----: |  :---- |  :---- |  :---- |  :---- |\n",
    "| **Layer 1** |  $(n^{[1]},12288)$ | $(n^{[1]},1)$  | $Z^{[1]} = W^{[1]}  X + b^{[1]}$ | $(n^{[1]},209)$ |\n",
    "| **Layer 1** |  $A$ | $A$  | $A$ | $A$ |\n",
    "| **Layer 1** |  $A$ | $A$  | $A$ | $A$ |\n",
    "| **Layer 1** |  $A$ | $A$  | $A$ | $A$ |\n",
    "| **Layer 1** |  $A$ | $A$  | $A$ | $A$ |\n",
    "| **Layer 1** |  $A$ | $A$  | $A$ | $A$ |\n",
    "| **Layer 1** |  $A$ | $A$  | $A$ | $A$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef3dc9e",
   "metadata": {},
   "source": [
    "\n",
    "Remember that when we compute $W X + b$ in python, it carries out broadcasting. For example, if: \n",
    "\n",
    "$$ W = \\begin{bmatrix}\n",
    "    j  & k  & l\\\\\n",
    "    m  & n & o \\\\\n",
    "    p  & q & r \n",
    "\\end{bmatrix}\\;\\;\\; X = \\begin{bmatrix}\n",
    "    a  & b  & c\\\\\n",
    "    d  & e & f \\\\\n",
    "    g  & h & i \n",
    "\\end{bmatrix} \\;\\;\\; b =\\begin{bmatrix}\n",
    "    s  \\\\\n",
    "    t  \\\\\n",
    "    u\n",
    "\\end{bmatrix}\\tag{2}$$\n",
    "\n",
    "Then $WX + b$ will be:\n",
    "\n",
    "$$ WX + b = \\begin{bmatrix}\n",
    "    (ja + kd + lg) + s  & (jb + ke + lh) + s  & (jc + kf + li)+ s\\\\\n",
    "    (ma + nd + og) + t & (mb + ne + oh) + t & (mc + nf + oi) + t\\\\\n",
    "    (pa + qd + rg) + u & (pb + qe + rh) + u & (pc + qf + ri)+ u\n",
    "\\end{bmatrix}\\tag{3}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afe1c1d",
   "metadata": {},
   "source": [
    "**练习**：为一个L层神经网络实现初始化。请您实现`initial.py`中的 `initialize_parameters_deep` 函数。\n",
    "\n",
    "**指导**：\n",
    "- 模型的结构是*[线性 -> ReLU] $ \\times$ (L-1) -> 线性 -> Sigmoid*。也就是说，它有$L-1$层使用ReLU激活函数，然后是使用sigmoid激活函数的输出层。\n",
    "- 对权重矩阵使用随机初始化。使用`np.random.randn(shape) * 0.01`。\n",
    "- 对偏置使用零初始化。使用`np.zeros(shape)`。\n",
    "- 我们将在变量`layer_dims`中存储$n^{[l]}$，即不同层中的单元数。例如，上周“平面数据分类模型”的`layer_dims`应该是[2,4,1]：有两个输入，一个具有4个隐藏单元的隐藏层，以及一个具有1个输出单元的输出层。这意味着`W1`的形状是(4,2)，`b1`是(4,1)，`W2`是(1,4)而`b2`是(1,1)。现在你将这个概念推广到$L$层！\n",
    "- 这是$L=1$情况（一个层神经网络）的实现。它应该能激发你实现一般情况（L层神经网络）。\n",
    "\n",
    "```python\n",
    "    if L == 1:\n",
    "        parameters[\"W\" + str(L)] = np.random.randn(layer_dims[1], layer_dims[0]) * 0.01\n",
    "        parameters[\"b\" + str(L)] = np.zeros((layer_dims[1], 1))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4b6c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from initial import initialize_parameters_deep\n",
    "\n",
    "parameters = initialize_parameters_deep([5,4,3])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53771ebe",
   "metadata": {},
   "source": [
    "**预期得到的输出**:\n",
    "    \n",
    "```\n",
    "W1 = [[ 0.72642933 -0.27358579 -0.23620559 -0.47984616  0.38702206]\n",
    " [-1.0292794   0.78030354 -0.34042208  0.14267862 -0.11152182]\n",
    " [ 0.65387455 -0.92132293 -0.14418936 -0.17175433  0.50703711]\n",
    " [-0.49188633 -0.07711224 -0.39259022  0.01887856  0.26064289]]\n",
    "b1 = [[0.]\n",
    " [0.]\n",
    " [0.]\n",
    " [0.]]\n",
    "W2 = [[-0.55030959  0.57236185  0.45079536  0.25124717]\n",
    " [ 0.45042797 -0.34186393 -0.06144511 -0.46788472]\n",
    " [-0.13394404  0.26517773 -0.34583038 -0.19837676]]\n",
    "b2 = [[0.]\n",
    " [0.]\n",
    " [0.]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fb41cd",
   "metadata": {},
   "source": [
    "# 7 - 梯度下降\n",
    "\n",
    "在这一部分，你将使用梯度下降更新模型的参数：\n",
    "\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{16}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{17}$$\n",
    "\n",
    "其中 $\\alpha$ 是学习率。在计算出更新后的参数后，将它们存储在参数字典中。\n",
    "\n",
    "**练习**：请您实现`optim.py`中的 `gradient_descent` 函数，使用梯度下降更新你的参数。\n",
    "\n",
    "**指导**：\n",
    "对每个 $W^{[l]}$ 和 $b^{[l]}$ 使用梯度下降进行更新，其中 $l = 1, 2, ..., L$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dceb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optim import gradient_descent\n",
    "from test_cases import gradient_descent_test_case\n",
    "\n",
    "parameters, grads = update_parameters_test_case()\n",
    "parameters = gradient_descent(parameters, grads, 0.1)\n",
    "\n",
    "print (\"W1 = \"+ str(parameters[\"W1\"]))\n",
    "print (\"b1 = \"+ str(parameters[\"b1\"]))\n",
    "print (\"W2 = \"+ str(parameters[\"W2\"]))\n",
    "print (\"b2 = \"+ str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8ef285",
   "metadata": {},
   "source": [
    "**预期得到的输出**:\n",
    "    \n",
    "```\n",
    "W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
    " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
    " [-1.0535704  -0.86128581  0.68284052  2.20374577]]\n",
    "b1 = [[-0.04659241]\n",
    " [-1.28888275]\n",
    " [ 0.53405496]]\n",
    "W2 = [[-0.55569196  0.0354055   1.32964895]]\n",
    "b2 = [[-0.84610769]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f0cb1e",
   "metadata": {},
   "source": [
    "# 8 - 总结\n",
    "\n",
    "恭喜您成功实现了构建深度神经网络所需的所有函数！\n",
    "\n",
    "我们知道这是一个漫长的任务，但未来的任务只会变得更好。下一个任务的难度会降低。\n",
    "\n",
    "在下一个任务中，您将把所有这些函数结合起来构建两个模型：\n",
    "\n",
    "- 一个两层神经网络\n",
    "- 一个 L 层神经网络\n",
    "\n",
    "请用Jupyter Notebook打来 `Application.ipynb`进入下一个lab，您将使用这些模型来对猫和非猫图像进行分类！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
